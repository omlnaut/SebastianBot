{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c81760",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/workspaces/SebastianBot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50eb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud.dependencies.clients import resolve_reddit_client\n",
    "\n",
    "\n",
    "client = resolve_reddit_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3789d129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RedditPost(subreddit=Subreddit(display_name='Python'), created_at=datetime.datetime(2025, 12, 22, 20, 28, 3, tzinfo=datetime.timezone.utc), title='What‚Äôs the slowest Python script you‚Äôre dealing with right now?', flair='Discussion', destination_url='https://www.reddit.com/r/Python/comments/1pta1k7/whats_the_slowest_python_script_youre_dealing/', text='Examples from recent clients:\\n\\n‚Ä¢  One guy‚Äôs reporting script went from 38s to 0.41s (93x)\\n\\n‚Ä¢  Another‚Äôs e-commerce ETL from 65s to 0.58s (112x)\\n\\n‚Ä¢  A daily analytics job from 51s to 0.44s (116x)\\n\\nAll with identical output ‚Äî no cheating or cutting corners.\\n\\nSo I‚Äôm curious ‚Äî what‚Äôs the slowest thing you‚Äôre running right now?\\n\\nHow many seconds/minutes does it take, and what does it do (roughly)?\\n\\nDrop it in the comments or DM me. I‚Äôll give you a quick gut check on what speedup is usually possible ‚Äî free', comments=[RedditComment(text='What are common causes?\\xa0', created_at=datetime.datetime(2025, 12, 22, 20, 31, 33, tzinfo=datetime.timezone.utc)), RedditComment(text='Slop', created_at=datetime.datetime(2025, 12, 22, 20, 40, 3, tzinfo=datetime.timezone.utc)), RedditComment(text='I inherited an application many, many years back that I was never quite able to nail down the big-O notation for due to its insane complexity. I *think* it was O(n log n). The home page would load every data point (yes everything). And for each data point it would query the entire database to see if the user had access to a single thing, but it went 7-13 layers deep depending on the type of object. Most of this was recursively checking for user access (again and again and again. The home page took 90-150 seconds to load depending on backend load (a fair amount going on). The database had something like 700 data points in it. I wrapped it in a a profiler and counted around 70-80k function calls just to look at a page for a single object.\\n\\nJob one was cleaning all of that up. Got page load time down to around 5-8 seconds without breaking any of the security constraints. It was my first big software project. The existing team had thrown their hands into the air for years trying to refactor the codebase and had given up long before i got there.', created_at=datetime.datetime(2025, 12, 22, 20, 44, 37, tzinfo=datetime.timezone.utc)), RedditComment(text='At work I completely reworked a script that stored a few plain CSV files in star scheme in a postgres DB. About 200.000 lines with 6 joined tables. The naive version using SQLModel (SQLAlchemy + Pydantic) took a few hours. Then I replaced everything in Polars and pre-calculated foreign keys. That version took 2 minutes but needed 5 GB ram. The newest Polars lazyframe version only requires 2 GB ram. The same custom Polars Io source with full Pydantic validation eats a CSV with 8 million lines for lunch.', created_at=datetime.datetime(2025, 12, 22, 20, 55, 13, tzinfo=datetime.timezone.utc)), RedditComment(text='Yeah man, the usual suspects I run into all the time when people send me their slow scripts:\\n\\n1.  Loops over big data ‚Äì like for row in df.iterrows() or nested for-loops on lists with 10k+ items. Kills performance because Python loops are slow as hell compared to vectorized stuff.\\n\\n2.  Chaining a bunch of pandas operations without assigning ‚Äì every filter or groupby makes a full copy if you‚Äôre not careful, and it adds up quick on bigger datasets.\\n\\n3.  Reading/writing files the wrong way ‚Äì default pd.read_csv on huge files without chunksize, or writing row-by-row instead of in batches.\\n\\n4.  Too many copies of big objects ‚Äì passing giant DataFrames around functions, or using .copy() everywhere when you don‚Äôt need to.\\n\\n5.  String concatenation in loops ‚Äì doing result += something a million times still builds up quadratic time, even with f-strings.\\n\\n6.  Using .apply with a custom python function ‚Äì basically just a disguised slow loop.\\n\\n7.  Loading way more data than you actually need ‚Äì pulling in whole CSVs when you only care about a few columns.\\n\\n8.  Wrong tool for the job ‚Äì using pandas for pure number crunching when NumPy would be faster, or lists when you should be using sets/dicts.\\nPretty much every slow script has 2-3 of these. Fixing the top couple usually gets you 10-30x, fixing more can push 80-100x+.\\nIf you‚Äôve got something that‚Äôs dragging, feel free to paste a snippet or just describe what it‚Äôs doing and how long it takes  . I‚Äôll tell you which of these are probably biting you. No charge or anything.', created_at=datetime.datetime(2025, 12, 22, 20, 37, 5, tzinfo=datetime.timezone.utc)), RedditComment(text=\"99.9% of the time it's using pandas data frame or numpy array like they are lists\", created_at=datetime.datetime(2025, 12, 22, 20, 56, 18, tzinfo=datetime.timezone.utc)), RedditComment(text=\"shooting from the hip here, but the usual suspects are:\\n\\n1. doing things sequentially which can be parallelized or done concurrently\\n\\n2. poorly coded retry logic, including excessively long timeouts\\n\\n3. non-optimal database interaction patterns, especially the case with large datasets, and especially extra the case with operations requiring locks that aren't handled gracefully\\n\\n4. doing repeated CPU intensive operations entirely in Python instead of using a library with bindings to an appropriate tool coded in lower level, more performant language. \\n\\n5. failing to use a cache where appropriate, resulting in repeated operations that didn't need to be repeated\", created_at=datetime.datetime(2025, 12, 22, 20, 41, 38, tzinfo=datetime.timezone.utc)), RedditComment(text='You give me code and I‚Äôll just show you, otherwise you just hating for no reason', created_at=datetime.datetime(2025, 12, 22, 20, 41, 31, tzinfo=datetime.timezone.utc)), RedditComment(text='You right I agree.', created_at=datetime.datetime(2025, 12, 22, 20, 45, 1, tzinfo=datetime.timezone.utc))]),\n",
       " RedditPost(subreddit=Subreddit(display_name='Python'), created_at=datetime.datetime(2025, 12, 22, 19, 25, 25, tzinfo=datetime.timezone.utc), title='An easy way to break an email or url into its component parts: Pyrolysate', flair='Showcase', destination_url='https://www.reddit.com/r/Python/comments/1pt8h37/an_easy_way_to_break_an_email_or_url_into_its/', text=\"About a year ago, I had a simple question that I wanted to answer: Can I break emails and URLs into their component parts?\\n\\nThis project was meant to be an easy afternoon project, maybe a weekend project, that taught me a few things about email parsing, URL parsing, and python standard libraries. It was only after starting this project that I learnt all of the complexities specifically in different URL formats.\\n\\n# What My Project Does\\n\\nPyrolysate is a Python library and CLI tool for parsing and validating URLs and email addresses. It breaks down URLs and emails into their component parts, validates against IANA's official TLD list, and outputs structured data in JSON, CSV, or text format.\\n\\n* Support for using files as inputs\\n* CLI available\\n* Compressed file and zip archive parsing support\\n* Converts to JSON object and JSON file\\n* Converts to CSV object and CSV file\\n\\n# Target Audience\\n\\n* Anyone who needs to have structured output for their emails and/or URLs\\n\\n# Comparison\\n\\n* Similar to urllib.parse but with more features\\n\\n# Links\\n\\n* GitHub:\\xa0[https://github.com/lignum-vitae/pyrolysate](https://github.com/lignum-vitae/pyrolysate)\\n\\n# Feedback I‚Äôd love\\n\\n* Project layout\\n* Code style improvements\\n* CLI command design\", comments=[]),\n",
       " RedditPost(subreddit=Subreddit(display_name='Python'), created_at=datetime.datetime(2025, 12, 22, 18, 2, 32, tzinfo=datetime.timezone.utc), title='Built a terminal-based encrypted vault in Python (learning project): PassFX', flair='Showcase', destination_url='https://www.reddit.com/r/Python/comments/1pt6be5/built_a_terminalbased_encrypted_vault_in_python/', text='Hi r/Python!   \\n  \\nI‚Äôm sharing a small side project I built to learn about CLI UX and local encrypted storage in Python.\\n\\n**Important note:** this is a learning/side project and **has not** been independently security-audited. I‚Äôm not recommending it for high-stakes use.  I‚Äôm mainly looking for feedback on Python structure, packaging, and CLI design.\\n\\n# What My Project Does\\n\\nPassFX is a terminal app that stores **text secrets locally** in an encrypted file and lets you:\\n\\n* add / view / update entries\\n* search by name/tag\\n* store notes like API keys, recovery codes, PINs, etc.\\n\\nIt‚Äôs designed to be keyboard-driven and fast, with the goal of a clean ‚Äúapp-like‚Äù CLI workflow.\\n\\n# Target Audience\\n\\n* Python developers who like building/using CLI tools\\n* Anyone curious about implementing encrypted local persistence + a searchable CLI UI in Python\\n* Not intended for production / ‚Äústore your crown jewels‚Äù usage unless it‚Äôs been properly reviewed/audited\\n\\n# Comparison\\n\\n* Unlike cloud-synced managers, this is **local-only** (no accounts, no sync).\\n* Unlike browser-based vaults, it‚Äôs **terminal-native**.\\n* Compared to `pass` (the Unix password store), I‚Äôm aiming for a more structured/interactive CLI flow (search + fields + notes), while keeping everything local.\\n\\n# Links\\n\\n* GitHub: [https://github.com/dinesh-git17/passfx](https://github.com/dinesh-git17/passfx)\\n* (Optional) project page: [https://passfx.dineshd.dev](https://passfx.dineshd.dev)\\n\\n# Feedback I‚Äôd love\\n\\n* Python packaging/project layout\\n* CLI command design + UX\\n* Testing approach for a CLI like this\\n* ‚ÄúGotchas‚Äù I should be aware of when building encrypted local storage (high-level guidance)', comments=[RedditComment(text='Hi there, from the /r/Python mods.\\n\\nWe want to emphasize that while security-centric programs are fun project spaces to explore we do not recommend that they be treated as a \\nsecurity solution unless they‚Äôve been audited by a third party, security professional and the audit is visible for review.\\n\\nSecurity is not easy. And making project to learn how to manage it is a great idea to learn about the complexity of this world. That said, there‚Äôs a difference \\nbetween exploring and learning about a topic space, and trusting that a product is secure for sensitive materials in the face of adversaries. \\n\\nWe hope you enjoy projects like these from a safety conscious perspective. \\n\\nWarm regards and all the best for your future Pythoneering,\\n\\n/r/Python moderator team\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Python) if you have any questions or concerns.*', created_at=datetime.datetime(2025, 12, 22, 18, 2, 33, tzinfo=datetime.timezone.utc)), RedditComment(text='Ha nice! i‚Äôve built https://psst.sh yesterday :)', created_at=datetime.datetime(2025, 12, 22, 18, 24, 39, tzinfo=datetime.timezone.utc)), RedditComment(text=\"This is a clean looking project! Building a vault is the best way to learn the hard parts of file I/O and serialization.\\n\\nRegarding the\\xa0**'Gotchas'**\\xa0for encryption in Python:\\n\\n1. **Memory Hygiene (The big one):**\\xa0Python strings are immutable. Unlike C/Rust, you cannot easily\\xa0`memset`\\xa0a variable to zero after using it. The plaintext password often lingers in memory until the Garbage Collector decides to clean it up. For a learning project, this is fine, but for production, this is why many vaults use Python only as a wrapper around C-extensions or use\\xa0`bytearray`\\xa0(mutable) where possible.\\n2. **Key Derivation:**\\xa0Ensure you aren't just hashing the master password. Use a proper KDF (Key Derivation Function) like\\xa0**Argon2id**\\xa0or\\xa0**PBKDF2**\\xa0with a unique salt per file to resist rainbow table attacks.\\n\\n**For Testing:**\\xa0If you built this with\\xa0`Click`\\xa0or\\xa0`Typer`, they have built-in testing runners that mock\\xa0`stdin/stdout`\\xa0beautifully. If you used\\xa0`argparse`, look into\\xa0`pexpect`\\xa0to script the terminal interactions for your integration tests.\", created_at=datetime.datetime(2025, 12, 22, 18, 27, 38, tzinfo=datetime.timezone.utc)), RedditComment(text='Love it! I‚Äôll give it a try', created_at=datetime.datetime(2025, 12, 22, 18, 26, 25, tzinfo=datetime.timezone.utc)), RedditComment(text='Thank you so much for this! Really appreciate you taking the time to write such a thoughtful response.\\n\\nThe point about memory hygiene is especially helpful. I was aware of the immutable string issue at a high level, but the way you framed it (Python as a wrapper vs. C extensions / `bytearray`) really clicked. Definitely something I‚Äôll keep in mind as the project evolves, even if it stays in ‚Äúlearning tool‚Äù territory for now.\\n\\nGood call on KDFs as well!!  I‚Äôm using a proper KDF with salt, but this is a great reminder to be explicit and careful there.\\n\\nThanks again üôèüèΩ  feedback like this is exactly why I wanted to share it here', created_at=datetime.datetime(2025, 12, 22, 18, 31, 39, tzinfo=datetime.timezone.utc))])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_posts(\"python\", limit=10)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3704ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = client.get_posts(\"python\", limit=30)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3c8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda p: len(p.comments), ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df47dd6",
   "metadata": {},
   "source": [
    "# Fetch recent comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8698e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = client._reddit.subreddit(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18c67e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(sub.comments(limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f086dca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4c2b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING_COMMENT_MESSAGE\n",
      "STR_FIELD\n",
      "all_awardings\n",
      "approved_at_utc\n",
      "approved_by\n",
      "archived\n",
      "associated_award\n",
      "author\n",
      "author_flair_background_color\n",
      "author_flair_css_class\n",
      "author_flair_richtext\n",
      "author_flair_template_id\n",
      "author_flair_text\n",
      "author_flair_text_color\n",
      "author_flair_type\n",
      "author_fullname\n",
      "author_is_blocked\n",
      "author_patreon_flair\n",
      "author_premium\n",
      "award\n",
      "awarders\n",
      "banned_at_utc\n",
      "banned_by\n",
      "block\n",
      "body\n",
      "body_html\n",
      "can_gild\n",
      "can_mod_post\n",
      "clear_vote\n",
      "collapse\n",
      "collapsed\n",
      "collapsed_because_crowd_control\n",
      "collapsed_reason\n",
      "collapsed_reason_code\n",
      "comment_type\n",
      "controversiality\n",
      "created\n",
      "created_utc\n",
      "delete\n",
      "disable_inbox_replies\n",
      "distinguished\n",
      "downs\n",
      "downvote\n",
      "edit\n",
      "edited\n",
      "enable_inbox_replies\n",
      "fullname\n",
      "gild\n",
      "gilded\n",
      "gildings\n",
      "id\n",
      "id_from_url\n",
      "is_root\n",
      "is_submitter\n",
      "likes\n",
      "link_author\n",
      "link_id\n",
      "link_permalink\n",
      "link_title\n",
      "link_url\n",
      "locked\n",
      "mark_read\n",
      "mark_unread\n",
      "mod\n",
      "mod_note\n",
      "mod_reason_by\n",
      "mod_reason_title\n",
      "mod_reports\n",
      "name\n",
      "no_follow\n",
      "num_comments\n",
      "num_reports\n",
      "over_18\n",
      "parent\n",
      "parent_id\n",
      "parse\n",
      "permalink\n",
      "quarantine\n",
      "refresh\n",
      "removal_reason\n",
      "replies\n",
      "reply\n",
      "report\n",
      "report_reasons\n",
      "save\n",
      "saved\n",
      "score\n",
      "score_hidden\n",
      "send_replies\n",
      "stickied\n",
      "submission\n",
      "subreddit\n",
      "subreddit_id\n",
      "subreddit_name_prefixed\n",
      "subreddit_type\n",
      "top_awarded_type\n",
      "total_awards_received\n",
      "treatment_tags\n",
      "unblock_subreddit\n",
      "uncollapse\n",
      "unrepliable_reason\n",
      "unsave\n",
      "ups\n",
      "upvote\n",
      "user_reports\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([attr for attr in dir(comments[0]) if not attr.startswith('_')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a778d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
